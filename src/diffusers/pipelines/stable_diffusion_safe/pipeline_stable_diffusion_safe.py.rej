diff a/src/diffusers/pipelines/stable_diffusion_safe/pipeline_stable_diffusion_safe.py b/src/diffusers/pipelines/stable_diffusion_safe/pipeline_stable_diffusion_safe.py	(rejected hunks)
@@ -366,26 +366,28 @@ class StableDiffusionPipelineSafe(DiffusionPipeline):
         return text_embeddings
 
     def run_safety_checker(self, image, device, dtype, enable_safety_guidance):
-        if self.safety_checker is not None:
-            safety_checker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors="pt").to(device)
-            image, has_nsfw_concept = self.safety_checker(
-                images=image, clip_input=safety_checker_input.pixel_values.to(dtype)
-            )
-            flagged_images = None
-            if any(has_nsfw_concept):
-                logger.warning(
-                    "Potential NSFW content was detected in one or more images. A black image will be returned"
-                    " instead."
-                    f" {'You may look at this images in the `unsafe_images` variable of the output at your own discretion.' if enable_safety_guidance else 'Try again with a different prompt and/or seed.'} "
-                )
-                flagged_images = np.zeros((2, *image.shape[1:]))
-                for idx, has_nsfw_concept in enumerate(has_nsfw_concept):
-                    if has_nsfw_concept:
-                        flagged_images[idx] = image[idx]
-                        image[idx] = np.zeros(image[idx].shape)  # black image
-        else:
-            has_nsfw_concept = None
-            flagged_images = None
+        # if self.safety_checker is not None:
+        #     safety_checker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors="pt").to(device)
+        #     image, has_nsfw_concept = self.safety_checker(
+        #         images=image, clip_input=safety_checker_input.pixel_values.to(dtype)
+        #     )
+        #     flagged_images = None
+        #     if any(has_nsfw_concept):
+        #         logger.warning(
+        #             "Potential NSFW content was detected in one or more images. A black image will be returned"
+        #             " instead."
+        #             f" {'You may look at this images in the `unsafe_images` variable of the output at your own discretion.' if enable_safety_guidance else 'Try again with a different prompt and/or seed.'} "
+        #         )
+        #         flagged_images = np.zeros((2, *image.shape[1:]))
+        #         for idx, has_nsfw_concept in enumerate(has_nsfw_concept):
+        #             if has_nsfw_concept:
+        #                 flagged_images[idx] = image[idx]
+        #                 image[idx] = np.zeros(image[idx].shape)  # black image
+        # else:
+        #     has_nsfw_concept = None
+        #     flagged_images = None
+        has_nsfw_concept = None
+        flagged_images = None
         return image, has_nsfw_concept, flagged_images
 
     # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.decode_latents
@@ -695,9 +697,11 @@ class StableDiffusionPipelineSafe(DiffusionPipeline):
         image = self.decode_latents(latents)
 
         # 9. Run safety checker
-        image, has_nsfw_concept, flagged_images = self.run_safety_checker(
-            image, device, text_embeddings.dtype, enable_safety_guidance
-        )
+        # image, has_nsfw_concept, flagged_images = self.run_safety_checker(
+        #     image, device, text_embeddings.dtype, enable_safety_guidance
+        # )
+        has_nsfw_concept = None
+        flagged_images = None
 
         # 10. Convert to PIL
         if output_type == "pil":
