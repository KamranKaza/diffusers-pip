diff a/src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion.py b/src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion.py	(rejected hunks)
@@ -81,6 +81,8 @@ class OnnxStableDiffusionPipeline(DiffusionPipeline):
             new_config["clip_sample"] = False
             scheduler._internal_dict = FrozenDict(new_config)
 
+        safety_checker = None
+
         self.register_modules(
             vae_encoder=vae_encoder,
             vae_decoder=vae_decoder,
@@ -280,24 +282,25 @@ class OnnxStableDiffusionPipeline(DiffusionPipeline):
         image = np.clip(image / 2 + 0.5, 0, 1)
         image = image.transpose((0, 2, 3, 1))
 
-        if self.safety_checker is not None:
-            safety_checker_input = self.feature_extractor(
-                self.numpy_to_pil(image), return_tensors="np"
-            ).pixel_values.astype(image.dtype)
-
-            image, has_nsfw_concepts = self.safety_checker(clip_input=safety_checker_input, images=image)
-
-            # There will throw an error if use safety_checker batchsize>1
-            images, has_nsfw_concept = [], []
-            for i in range(image.shape[0]):
-                image_i, has_nsfw_concept_i = self.safety_checker(
-                    clip_input=safety_checker_input[i : i + 1], images=image[i : i + 1]
-                )
-                images.append(image_i)
-                has_nsfw_concept.append(has_nsfw_concept_i[0])
-            image = np.concatenate(images)
-        else:
-            has_nsfw_concept = None
+        # if self.safety_checker is not None:
+        #     safety_checker_input = self.feature_extractor(
+        #         self.numpy_to_pil(image), return_tensors="np"
+        #     ).pixel_values.astype(image.dtype)
+
+        #     image, has_nsfw_concepts = self.safety_checker(clip_input=safety_checker_input, images=image)
+
+        #     # There will throw an error if use safety_checker batchsize>1
+        #     images, has_nsfw_concept = [], []
+        #     for i in range(image.shape[0]):
+        #         image_i, has_nsfw_concept_i = self.safety_checker(
+        #             clip_input=safety_checker_input[i : i + 1], images=image[i : i + 1]
+        #         )
+        #         images.append(image_i)
+        #         has_nsfw_concept.append(has_nsfw_concept_i[0])
+        #     image = np.concatenate(images)
+        # else:
+        #     has_nsfw_concept = None
+        has_nsfw_concept = None
 
         if output_type == "pil":
             image = self.numpy_to_pil(image)
