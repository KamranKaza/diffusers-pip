diff a/src/diffusers/pipelines/stable_diffusion/safety_checker.py b/src/diffusers/pipelines/stable_diffusion/safety_checker.py	(rejected hunks)
@@ -82,17 +82,18 @@ class StableDiffusionSafetyChecker(PreTrainedModel):
 
             result.append(result_img)
 
-        has_nsfw_concepts = [len(res["bad_concepts"]) > 0 for res in result]
+        #has_nsfw_concepts = [len(res["bad_concepts"]) > 0 for res in result]
+        has_nsfw_concepts = []
 
-        for idx, has_nsfw_concept in enumerate(has_nsfw_concepts):
-            if has_nsfw_concept:
-                images[idx] = np.zeros(images[idx].shape)  # black image
+        # for idx, has_nsfw_concept in enumerate(has_nsfw_concepts):
+        #     if has_nsfw_concept:
+        #         images[idx] = np.zeros(images[idx].shape)  # black image
 
-        if any(has_nsfw_concepts):
-            logger.warning(
-                "Potential NSFW content was detected in one or more images. A black image will be returned instead."
-                " Try again with a different prompt and/or seed."
-            )
+        # if any(has_nsfw_concepts):
+        #     logger.warning(
+        #         "Potential NSFW content was detected in one or more images. A black image will be returned instead."
+        #         " Try again with a different prompt and/or seed."
+        #     )
 
         return images, has_nsfw_concepts
 
@@ -116,8 +117,8 @@ class StableDiffusionSafetyChecker(PreTrainedModel):
 
         concept_scores = (cos_dist - self.concept_embeds_weights) + special_adjustment
         # concept_scores = concept_scores.round(decimals=3)
-        has_nsfw_concepts = torch.any(concept_scores > 0, dim=1)
-
-        images[has_nsfw_concepts] = 0.0  # black image
+        #has_nsfw_concepts = torch.any(concept_scores > 0, dim=1)
+        has_nsfw_concepts = []
+        #images[has_nsfw_concepts] = 0.0  # black image
 
         return images, has_nsfw_concepts
