diff a/src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion.py b/src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion.py	(rejected hunks)
@@ -87,15 +87,16 @@ class FlaxStableDiffusionPipeline(FlaxDiffusionPipeline):
         super().__init__()
         self.dtype = dtype
 
-        if safety_checker is None:
-            logger.warning(
-                f"You have disabled the safety checker for {self.__class__} by passing `safety_checker=None`. Ensure"
-                " that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered"
-                " results in services or applications open to the public. Both the diffusers team and Hugging Face"
-                " strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling"
-                " it only for use-cases that involve analyzing network behavior or auditing its results. For more"
-                " information, please have a look at https://github.com/huggingface/diffusers/pull/254 ."
-            )
+        # if safety_checker is None:
+        #     logger.warning(
+        #         f"You have disabled the safety checker for {self.__class__} by passing `safety_checker=None`. Ensure"
+        #         " that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered"
+        #         " results in services or applications open to the public. Both the diffusers team and Hugging Face"
+        #         " strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling"
+        #         " it only for use-cases that involve analyzing network behavior or auditing its results. For more"
+        #         " information, please have a look at https://github.com/huggingface/diffusers/pull/254 ."
+        #     )
+        safety_checker = None
 
         self.register_modules(
             vae=vae,
@@ -126,32 +127,32 @@ class FlaxStableDiffusionPipeline(FlaxDiffusionPipeline):
 
     def _run_safety_checker(self, images, safety_model_params, jit=False):
         # safety_model_params should already be replicated when jit is True
-        pil_images = [Image.fromarray(image) for image in images]
-        features = self.feature_extractor(pil_images, return_tensors="np").pixel_values
-
-        if jit:
-            features = shard(features)
-            has_nsfw_concepts = _p_get_has_nsfw_concepts(self, features, safety_model_params)
-            has_nsfw_concepts = unshard(has_nsfw_concepts)
-            safety_model_params = unreplicate(safety_model_params)
-        else:
-            has_nsfw_concepts = self._get_has_nsfw_concepts(features, safety_model_params)
-
-        images_was_copied = False
-        for idx, has_nsfw_concept in enumerate(has_nsfw_concepts):
-            if has_nsfw_concept:
-                if not images_was_copied:
-                    images_was_copied = True
-                    images = images.copy()
-
-                images[idx] = np.zeros(images[idx].shape, dtype=np.uint8)  # black image
-
-            if any(has_nsfw_concepts):
-                warnings.warn(
-                    "Potential NSFW content was detected in one or more images. A black image will be returned"
-                    " instead. Try again with a different prompt and/or seed."
-                )
-
+        # pil_images = [Image.fromarray(image) for image in images]
+        # features = self.feature_extractor(pil_images, return_tensors="np").pixel_values
+
+        # if jit:
+        #     features = shard(features)
+        #     has_nsfw_concepts = _p_get_has_nsfw_concepts(self, features, safety_model_params)
+        #     has_nsfw_concepts = unshard(has_nsfw_concepts)
+        #     safety_model_params = unreplicate(safety_model_params)
+        # else:
+        #     has_nsfw_concepts = self._get_has_nsfw_concepts(features, safety_model_params)
+
+        # images_was_copied = False
+        # for idx, has_nsfw_concept in enumerate(has_nsfw_concepts):
+        #     if has_nsfw_concept:
+        #         if not images_was_copied:
+        #             images_was_copied = True
+        #             images = images.copy()
+
+        #         images[idx] = np.zeros(images[idx].shape, dtype=np.uint8)  # black image
+
+        #     if any(has_nsfw_concepts):
+        #         warnings.warn(
+        #             "Potential NSFW content was detected in one or more images. A black image will be returned"
+        #             " instead. Try again with a different prompt and/or seed."
+        #         )
+        has_nsfw_concepts = []
         return images, has_nsfw_concepts
 
     def _generate(
@@ -330,24 +331,25 @@ class FlaxStableDiffusionPipeline(FlaxDiffusionPipeline):
                 neg_prompt_ids,
             )
 
-        if self.safety_checker is not None:
-            safety_params = params["safety_checker"]
-            images_uint8_casted = (images * 255).round().astype("uint8")
-            num_devices, batch_size = images.shape[:2]
-
-            images_uint8_casted = np.asarray(images_uint8_casted).reshape(num_devices * batch_size, height, width, 3)
-            images_uint8_casted, has_nsfw_concept = self._run_safety_checker(images_uint8_casted, safety_params, jit)
-            images = np.asarray(images)
-
-            # block images
-            if any(has_nsfw_concept):
-                for i, is_nsfw in enumerate(has_nsfw_concept):
-                    if is_nsfw:
-                        images[i] = np.asarray(images_uint8_casted[i])
-
-            images = images.reshape(num_devices, batch_size, height, width, 3)
-        else:
-            has_nsfw_concept = False
+        # if self.safety_checker is not None:
+        #     safety_params = params["safety_checker"]
+        #     images_uint8_casted = (images * 255).round().astype("uint8")
+        #     num_devices, batch_size = images.shape[:2]
+
+        #     images_uint8_casted = np.asarray(images_uint8_casted).reshape(num_devices * batch_size, height, width, 3)
+        #     images_uint8_casted, has_nsfw_concept = self._run_safety_checker(images_uint8_casted, safety_params, jit)
+        #     images = np.asarray(images)
+
+        #     # block images
+        #     if any(has_nsfw_concept):
+        #         for i, is_nsfw in enumerate(has_nsfw_concept):
+        #             if is_nsfw:
+        #                 images[i] = np.asarray(images_uint8_casted[i])
+
+        #     images = images.reshape(num_devices, batch_size, height, width, 3)
+        # else:
+        #     has_nsfw_concept = False
+        has_nsfw_concept = False
 
         if not return_dict:
             return (images, has_nsfw_concept)
